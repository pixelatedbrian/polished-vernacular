{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# which model are we going to use\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# text vectorizing stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# things to enable scoring and cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some helper stuff for tracking performance over the duration of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_timestamp(model_type, kfolds, scores, note):\n",
    "    '''\n",
    "    Parameters:\n",
    "    model_type = string description of the model(s) used to make these scores\n",
    "    kfolds     = how many folds in kfold cross validation used\n",
    "    scores     = list of ROC AUC avg scores of models for each class, floats should be like 0.9784\n",
    "    note       = string, whatever is of note about the model, made a change or whatever\n",
    "    \n",
    "    Returns:\n",
    "    None, but writes (appends) a line to scores.txt in the root directory so that progress can be tracked\n",
    "    The format is:\n",
    "            time(s)~model_type~kfold~avg_roc_auc~toxic_auc~s_toxic_auc~obscene_auc~threat_auc~insult_auc~i_hate_auc~notes\n",
    "            \n",
    "    scores.txt is a tilde '~' seperated CSV like:\n",
    "        time~model_type~kfold~avg_roc_auc~toxic_auc~s_toxic_auc~obscene_auc~threat_auc~insult_auc~i_hate_auc~notes\n",
    "        1520303252~0.9794005980274005~note something\n",
    "    '''\n",
    "\n",
    "    out_text = \"{:10.0f}~{:}~{:2d}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:}\\n\".format(time.time(), \n",
    "                                             model_type, \n",
    "                                             kfolds, \n",
    "                                             np.mean(scores),\n",
    "                                             scores[0],\n",
    "                                             scores[1],\n",
    "                                             scores[2],\n",
    "                                             scores[3],\n",
    "                                             scores[4],\n",
    "                                             scores[5],                                                \n",
    "                                             note)\n",
    "    \n",
    "    with open(\"../scores.txt\", 'a') as out_file:\n",
    "        out_file.write(out_text)\n",
    "        \n",
    "        print(\"wrote:\")\n",
    "        print(out_text)\n",
    "        print(\"to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and light processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../data/test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize words from both corpuses (corpi?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=50000)    # 10k was initial, 50k seemed to work well in initial testing\n",
    "\n",
    "word_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (159571, 50000)\n",
      "test shape: (153164, 50000)\n"
     ]
    }
   ],
   "source": [
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "print(\"train shape:\", train_word_features.shape)\n",
    "print(\"test shape:\", test_word_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to tune on a single split instead of 10 kfold since that will take forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: toxic           ROC AUC: 0.8265\n",
      "Class: severe_toxic    ROC AUC: 0.6872\n",
      "Class: obscene         ROC AUC: 0.8670\n",
      "Class: threat          ROC AUC: 0.5671\n",
      "Class: insult          ROC AUC: 0.8053\n",
      "Class: identity_hate   ROC AUC: 0.6244\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression:\n",
    "#   B-)\n",
    "# Multinomial NB:\n",
    "#   alpha=0.03\n",
    "#   fit_prior=False\n",
    "# RandomForests:\n",
    "#   n_estimators=32\n",
    "#   max_depth=512 seems to be the best with simple hparam testing\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for _class in class_names:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_word_features, train[_class], test_size=0.1, random_state=1337)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    \n",
    "#     print(\"Class: {:}\".format(_class))\n",
    "#     for nest in nests:\n",
    "    model1 = RandomForestClassifier(n_estimators=32, n_jobs=-1, max_depth=512)\n",
    "    model2 = MultinomialNB(alpha=0.03, fit_prior=False)\n",
    "    model3 = LogisticRegression(solver='sag')\n",
    "    \n",
    "    meta_model = VotingClassifier(estimators=[('rf', model1), ('mnb', model2), ('lr', model3)],\n",
    "                                  weights=[1.0, 1.0, 1.5],\n",
    "                                  voting='soft',\n",
    "                                  n_jobs=-1)\n",
    "    \n",
    "    meta_model.fit(x_train, y_train)\n",
    "\n",
    "    preds = meta_model.predict(x_test)\n",
    "\n",
    "    result = roc_auc_score(y_test, preds)\n",
    "    results.append(result)\n",
    "\n",
    "    print(\"Class: {: <14}  ROC AUC: {:0.4f}\".format(_class, result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting ensemble hard voting\n",
    "# Class: toxic           ROC AUC: 0.8192\n",
    "# Class: severe_toxic    ROC AUC: 0.6612\n",
    "# Class: obscene         ROC AUC: 0.8622\n",
    "# Class: threat          ROC AUC: 0.5766\n",
    "# Class: insult          ROC AUC: 0.8028\n",
    "# Class: identity_hate   ROC AUC: 0.6099\n",
    "\n",
    "# voting ensemble soft voting\n",
    "# Class: toxic           ROC AUC: 0.8360\n",
    "# Class: severe_toxic    ROC AUC: 0.7132\n",
    "# Class: obscene         ROC AUC: 0.8777\n",
    "# Class: threat          ROC AUC: 0.5863\n",
    "# Class: insult          ROC AUC: 0.8261\n",
    "# Class: identity_hate   ROC AUC: 0.6647\n",
    "\n",
    "# voting ensemble soft voting with weights [1.0, 1.0, 1.5]\n",
    "# Class: toxic           ROC AUC: 0.8265\n",
    "# Class: severe_toxic    ROC AUC: 0.6872\n",
    "# Class: obscene         ROC AUC: 0.8670\n",
    "# Class: threat          ROC AUC: 0.5671\n",
    "# Class: insult          ROC AUC: 0.8053\n",
    "# Class: identity_hate   ROC AUC: 0.6244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Spread for class \"toxic\":\n",
      "    0.9687     0.9668     0.9701     0.9695     0.9691     0.9717     0.9675     0.9692     0.9699     0.9708  \n",
      "    CV score for class \"toxic\" is 0.9693\n",
      "\n",
      "CV Spread for class \"severe_toxic\":\n",
      "    0.9819     0.9765     0.9695     0.9822     0.9843     0.9742     0.9784     0.9773     0.9845     0.9773  \n",
      "    CV score for class \"severe_toxic\" is 0.9786\n",
      "\n",
      "CV Spread for class \"obscene\":\n",
      "    0.9861     0.9843     0.9807     0.9816     0.9843     0.9785     0.9837     0.9863     0.9851     0.9815  \n",
      "    CV score for class \"obscene\" is 0.9832\n",
      "\n",
      "CV Spread for class \"threat\":\n",
      "    0.9568     0.9421     0.9694     0.9549     0.9532     0.9669     0.9763     0.9622     0.9580     0.9276  \n",
      "    CV score for class \"threat\" is 0.9567\n",
      "\n",
      "CV Spread for class \"insult\":\n",
      "    0.9747     0.9738     0.9729     0.9744     0.9774     0.9711     0.9736     0.9751     0.9775     0.9746  \n",
      "    CV score for class \"insult\" is 0.9745\n",
      "\n",
      "CV Spread for class \"identity_hate\":\n",
      "    0.9582     0.9593     0.9536     0.9686     0.9453     0.9648     0.9679     0.9717     0.9617     0.9650  \n",
      "    CV score for class \"identity_hate\" is 0.9616\n",
      "\n",
      "Total CV score is 0.9707\n",
      "wrote:\n",
      "1520414904~Stacked Voting Ensemble~10~0.97066580~0.96932158~0.97860966~0.98320126~0.95674089~0.97450542~0.96161602~word2vec max 50k features, Logistic Regression, MultinomialNB, and RandomForests\n",
      "\n",
      "to file\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "train_features = train_word_features.copy()\n",
    "\n",
    "# submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    \n",
    "    model1 = RandomForestClassifier(n_estimators=32, n_jobs=-1, max_depth=512)\n",
    "    model2 = MultinomialNB(alpha=0.03, fit_prior=False)\n",
    "    model3 = LogisticRegression(solver='sag')\n",
    "    \n",
    "    classifier = VotingClassifier(estimators=[('rf', model1), ('mnb', model2), ('lr', model3)],\n",
    "                                  voting='soft',\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=1337)\n",
    "    \n",
    "#     results = cross_val_score(classifier, train_features, train_target, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "    results = cross_val_score(classifier, train_features, train_target, cv=kfold, scoring='roc_auc')\n",
    "    \n",
    "    print('CV Spread for class \"{}\":'.format(class_name))\n",
    "    for result in results:\n",
    "        print(\"    {:0.4f}\".format(result), end=\" \")\n",
    "        \n",
    "    print(\" \")\n",
    "        \n",
    "    cv_score = np.mean(results)\n",
    "    scores.append(cv_score)\n",
    "    \n",
    "    print('    CV score for class \"{}\" is {:0.4}\\n'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "#     submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {:0.4f}'.format(np.mean(scores)))\n",
    "\n",
    "write_model_timestamp('Stacked Voting Ensemble', NUM_FOLDS, scores, \"word2vec max 50k features, Logistic Regression, MultinomialNB, and RandomForests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS          AVG ROC AUC\n",
      "toxic          0.9698\n",
      "severe_toxic   0.9859\n",
      "obscene        0.9854\n",
      "threat         0.9828\n",
      "insult         0.9765\n",
      "identity_hate  0.9761\n"
     ]
    }
   ],
   "source": [
    "print(\"{: <14} {:}\".format(\"CLASS\", \"AVG ROC AUC\"))\n",
    "\n",
    "for item in zip(class_names, scores):\n",
    "    print(\"{: <14} {:0.4f}\".format(item[0], item[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
