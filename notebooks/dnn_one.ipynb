{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11742289275102371150\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1869021184\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17979890597722925516\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import K\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some helper stuff for tracking performance over the duration of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_timestamp(model_type, kfolds, scores, note):\n",
    "    '''\n",
    "    Parameters:\n",
    "    model_type = string description of the model(s) used to make these scores\n",
    "    kfolds     = how many folds in kfold cross validation used\n",
    "    scores     = list of ROC AUC avg scores of models for each class, floats should be like 0.9784\n",
    "    note       = string, whatever is of note about the model, made a change or whatever\n",
    "    \n",
    "    Returns:\n",
    "    None, but writes (appends) a line to scores.txt in the root directory so that progress can be tracked\n",
    "    The format is:\n",
    "            time(s)~model_type~kfold~avg_roc_auc~toxic_auc~s_toxic_auc~obscene_auc~threat_auc~insult_auc~i_hate_auc~notes\n",
    "            \n",
    "    scores.txt is a tilde '~' seperated CSV like:\n",
    "        time~model_type~kfold~avg_roc_auc~toxic_auc~s_toxic_auc~obscene_auc~threat_auc~insult_auc~i_hate_auc~notes\n",
    "        1520303252~0.9794005980274005~note something\n",
    "    '''\n",
    "\n",
    "    out_text = \"{:10.0f}~{:}~{:2d}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:0.8f}~{:}\\n\".format(time.time(), \n",
    "                                             model_type, \n",
    "                                             kfolds, \n",
    "                                             np.mean(scores),\n",
    "                                             scores[0],\n",
    "                                             scores[1],\n",
    "                                             scores[2],\n",
    "                                             scores[3],\n",
    "                                             scores[4],\n",
    "                                             scores[5],                                                \n",
    "                                             note)\n",
    "    \n",
    "    with open(\"../scores.txt\", 'a') as out_file:\n",
    "        out_file.write(out_text)\n",
    "        \n",
    "        print(\"wrote:\")\n",
    "        print(out_text)\n",
    "        print(\"to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and light processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../data/test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize words from both corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=25000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=25000)    # 10k was initial\n",
    "\n",
    "word_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (159571, 25000)\n",
      "test shape: (153164, 25000)\n"
     ]
    }
   ],
   "source": [
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "print(\"train shape:\", train_word_features.shape)\n",
    "print(\"test shape:\", test_word_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a neural net from scratch, what could possibly go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(learning_rate=0.01, lr_decay=1e-6, drop_out=0.2, input_shape=(50000,)):\n",
    "    \n",
    "    DROPOUT = drop_out\n",
    "\n",
    "    # Input\n",
    "    _inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Try like 5 hidden layers starting big and getting small\n",
    "    \n",
    "    # maybe try kernel_initializer='he_normal' on dense layers\n",
    "    X = Dense(256, activation=\"relu\", name=\"dense_1\")(_inputs)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DROPOUT)(X)\n",
    "\n",
    "    X = Dense(128, activation=\"relu\", name=\"dense_2\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DROPOUT)(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", name=\"dense_3\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DROPOUT)(X)\n",
    "\n",
    "    X = Dense(32, activation=\"relu\", name=\"dense_4\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DROPOUT)(X)\n",
    "\n",
    "    X = Dense(16, activation=\"relu\", name=\"dense_5\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DROPOUT)(X)\n",
    "\n",
    "    _outputs = Dense(1, activation='sigmoid', name=\"output_1\")(X)\n",
    "\n",
    "    # gather model\n",
    "    model = Model(inputs=_inputs, outputs=_outputs, name=\"bad_dnn\")\n",
    "    \n",
    "    # configure optimizer\n",
    "    optimizer = Adam(lr=learning_rate, decay=lr_decay)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,6)),str(round(roc_val,6))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a single fold for experimentation since 10fold will take forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 25000)\n",
      "threat\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/7\n",
      "143613/143613 [==============================] - 31s 215us/step - loss: 0.9127 - acc: 0.5278 - val_loss: 0.6340 - val_acc: 0.7144\n",
      "Epoch 2/7\n",
      "143613/143613 [==============================] - 34s 237us/step - loss: 0.6253 - acc: 0.6765 - val_loss: 0.4496 - val_acc: 0.9959\n",
      "Epoch 3/7\n",
      "143613/143613 [==============================] - 34s 234us/step - loss: 0.4504 - acc: 0.8475 - val_loss: 0.3406 - val_acc: 0.9961\n",
      "Epoch 4/7\n",
      "143613/143613 [==============================] - 34s 234us/step - loss: 0.3154 - acc: 0.9514 - val_loss: 0.2321 - val_acc: 0.9967\n",
      "Epoch 5/7\n",
      "143613/143613 [==============================] - 34s 235us/step - loss: 0.2119 - acc: 0.9833 - val_loss: 0.1439 - val_acc: 0.9967\n",
      "Epoch 6/7\n",
      " 92160/143613 [==================>...........] - ETA: 11s - loss: 0.1559 - acc: 0.9904"
     ]
    }
   ],
   "source": [
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# for _class in class_names:\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_word_features, \n",
    "                                                    train[class_names[3]], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=1337)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(class_names[3])\n",
    "\n",
    "model = dnn_model(learning_rate=0.00025, drop_out=0.50, input_shape=(x_train.shape[1],))\n",
    "# print(model.summary())\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                batch_size=512,\n",
    "                epochs=7,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=[roc_callback(training_data=(x_train, y_train),validation_data=(x_test, y_test))])\n",
    "\n",
    "\n",
    "\n",
    "# for _class in class_names:\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(train_word_features, train[_class], test_size=0.1, random_state=1337)\n",
    "\n",
    "\n",
    "#     results = []\n",
    "    \n",
    "# #     print(\"Class: {:}\".format(_class))\n",
    "# #     for nest in nests:\n",
    "#     model1 = RandomForestClassifier(n_estimators=32, n_jobs=-1, max_depth=512)\n",
    "#     model2 = MultinomialNB(alpha=0.03, fit_prior=False)\n",
    "#     model3 = LogisticRegression(solver='sag')\n",
    "    \n",
    "#     meta_model = VotingClassifier(estimators=[('rf', model1), ('mnb', model2), ('lr', model3)],\n",
    "#                                   weights=[1.0, 1.0, 1.5],\n",
    "#                                   voting='soft',\n",
    "#                                   n_jobs=-1)\n",
    "    \n",
    "#     meta_model.fit(x_train, y_train)\n",
    "\n",
    "#     preds = meta_model.predict(x_test)\n",
    "\n",
    "#     result = roc_auc_score(y_test, preds)\n",
    "#     results.append(result)\n",
    "\n",
    "#     print(\"Class: {: <14}  ROC AUC: {:0.4f}\".format(_class, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "train_features = train_word_features.copy()\n",
    "\n",
    "# submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    \n",
    "    model1 = RandomForestClassifier(n_estimators=32, n_jobs=-1, max_depth=512)\n",
    "    model2 = MultinomialNB(alpha=0.03, fit_prior=False)\n",
    "    model3 = LogisticRegression(solver='sag')\n",
    "    \n",
    "    classifier = VotingClassifier(estimators=[('rf', model1), ('mnb', model2), ('lr', model3)],\n",
    "                                  voting='soft',\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=1337)\n",
    "    \n",
    "#     results = cross_val_score(classifier, train_features, train_target, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "    results = cross_val_score(classifier, train_features, train_target, cv=kfold, scoring='roc_auc')\n",
    "    \n",
    "    print('CV Spread for class \"{}\":'.format(class_name))\n",
    "    for result in results:\n",
    "        print(\"    {:0.4f}\".format(result), end=\" \")\n",
    "        \n",
    "    print(\" \")\n",
    "        \n",
    "    cv_score = np.mean(results)\n",
    "    scores.append(cv_score)\n",
    "    \n",
    "    print('    CV score for class \"{}\" is {:0.4}\\n'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "#     submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {:0.4f}'.format(np.mean(scores)))\n",
    "\n",
    "write_model_timestamp('DNN', NUM_FOLDS, scores, \"word2vec max 25k features, 5 FC layers 256->128->64>32->16->1 output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
